# Genomics-Annotation-Service

Objectives
Gain experience with deployment approaches and techniques for scaling and testing cloud applications, as well as integrating external SaaS providers. 
Context
See this document for a description of GAS functionality once the system is completed:
https://docs.google.com/document/d/1sE46IKwk1ZiaZvMvVndKuP4Kmxgw5VC1B4pkddLrI8I/edit?usp=sharing
Preparation
Please adhere to the guidelines below as you set up your environment for this project:

Very important: Terminate ALL your current instances and launch new ones. Before launching new instances, make a note of the Elastic IP address assigned to your web app instance (you can run: ping <aws-username>.ucmpcs.org).

I strongly recommend you use the following command to launch your instances:

aws ec2 create-tags --resources `aws ec2 run-instances --image-id ami-35688858 --count 1 --instance-type t2.micro --key-name <your_key_name> --security-groups mpcs --user-data https://s3.amazonaws.com/mpcs-resources/src/mpcs_capstone_user_data.txt --query 'Instances[0].InstanceId' --output text` --tags 'Key=Name,Value=<your_username>-Capstone'

This command uses a more recent AMI than the one we’ve been using (it has updated files that are required for the project). It also sends user data to initialize your instance. Yes, you can manually run all of these setup actions but you will need to automate them later in the project when experimenting with scaling, so it’s worthwhile spending the time to get it right now. You can view the user data here: https://s3.amazonaws.com/mpcs-resources/src/mpcs_capstone_user_data.txt.

Note that with this AMI the instances no longer use a virtualenv (it just makes the automated provisioning simpler for the purposes of this exercise). So now all the python modules are installed globally and you can import them directly after accessing the instance. 

Ensure that you use the mpcs security group when launching instances (as in the command above). Do not use your own group or the one automatically generated by EC2 when launching an instance.

After launching a new instance you will need to re-associate your Elastic IP address with the new instance. You can do this using the AWS console or by  running the following: 
aws ec2 associate-address --instance-id <your_inst> --public-ip <your_ip> 

When accessing your new instance via SSH for the first time, you will get a host identification error. Run this command to remove the old instance from your authorized hosts file: ssh-keygen -R <aws-username>.ucmpcs.org

Using the Enhanced Bottle Web Framework
We want the GAS to look somewhat appealing so we are going to use an enhanced version of the Bottle framework for this project. Note: You must use this updated framework because it contains required code for registering new users and managing authentication/authorization.

The enhanced web framework is here (it’s also downloaded and unzipped automatically as part of the user data passed to the instance when launching instances with the command above): https://s3.amazonaws.com/mpcs-resources/src/mpcs_capstone_web_framework.zip.

The enhanced framework provides the following:

Styled web pages using the Bootstrap CSS library (www.getbootstrap.com). You don’t need to change anything related to styling, but if you want to experiment with a different look and feel, have at it!

A modular template structure that allows every page in your application to share common headers, footers and other HTML elements. In order to use this modified structure you need to be aware of the following:

Your template (.tpl) files must reside in the views/ directory, off the root directory of your application.

You no longer need to include header and footer HTML in your template. Your templates will now look like this:

%include('views/header.tpl')

<div class="container">
    <div class="page-header">
        <h2>My Page</h2>
    </div>

    Your page content/form/etc. goes here....
    <div class="row">
		        Use <div class="row">...</div> to enclose page elements
                    and combine it with DIVs that specify column widths
                    to lay out your page. Bootstrap splits your web page into 12
                    equal-width columns so, for example, if you want content to
                    span one third of the page width you
                    can enclose it in <div class="col-md-4"> .... </div>
    </div>
    		</div> <!-- container -->

%rebase('templates/base', title='GAS - My Page')

The %rebase statement must be included as the last line in your template. This tells Bottle to replace a placeholder in the base template with the content of your template. See the home.tpl and login.tpl for two examples that you can repurpose for your use.

The framework uses the Bottle Config class to set and access various configuration values throughout your application. You can add and change configuration key/value pairs in mpcs.conf using standard .ini file format. Then you can access these in your code using dotted notation, e.g. request.app.config ['mpcs.aws.s3.inputs_bucket'] will return the value “gas-inputs” -- note that all config values are returned as strings.

If needed, the framework provides a logger that can be used to log events to a file and to the console. To use the logger, add this line to your source file: from mpcs_utils import log
and then you may write to the log like so: log.info("Server did something...") 

Runtime environment configuration parameters are stored in OS environment variables and read into the Bottle config at startup. These include important parameters like the location of static resources and the port that the application listens on. To change these parameters, edit the file mpcs_run.sh. The default file looks like this:

export MPCS_DEBUG=True
export MPCS_APP_HOST=0.0.0.0
export MPCS_APP_PORT=8888
export MPCS_STATIC_ROOT=./static/
export MPCS_TEMPLATES_ROOT=templates/
export MPCS_LOGS_ROOT=./log/
export MPCS_LOG_FILE=mpcs.log
[[ -d ./log ]] || mkdir $MPCS_LOGS_ROOT
if [ ! -e $MPCS_LOGS_ROOT/$MPCS_LOG_FILE ]; then
    touch $MPCS_LOGS_ROOT/$MPCS_LOG_FILE;
fi
python web_server.py

When using the enhanced framework, in order to start our web server we just do: ./mpcs_run.sh

In the enhanced framework the main script is called run_server.py. This file imports mpcs_app.py which is where your front end source code will be added. mpcs_app.py currently has code for just the home page and the registration/login forms.

IMPORTANT NOTES
Your existing code for uploading files, sending notifications to SNS, etc. must be moved into/added to the mpcs_app.py file.

Your existing upload.tpl template (and any other templates you created previously) must be converted to use the modular structure. Essentially, this means removing any header and footer HTML , and adding the %include and %rebase statements at the top and bottom of the file.

The enhanced framework also implements some rudimentary security. From this point on our web server will respond only to HTTPS requests. We require HTTPS for things like registering and authenticating users. Many web sites/apps use secure connections for a subset of their pages, but it is now considered a best practice to use HTTPS throughout the entire site/app. We will also substitute the default Bottle WSGI server with the CherryPy server which is a well-tested, multi-threaded WSGI server suitable for production use.

Browse web_server.py and see how it uses the CherryPy WSGI server. As previously discussed, the standard Bottle WSGI server is adequate for development but not for a production system (and it does not support HTTPS) so we have replaced it with something more robust. In addition, our application will now listen on port 4433 instead of 8888. 

Check that you have an SSL certificate and private key on your instance, in the main directory of your application. If you use the command listed above to launch your instance, these files will be automatically added as part of the user data passed to the instance. If you launched an instance using the console or other command you can download and unzip the SSL files from: https://s3.amazonaws.com/mpcs-resources/src/mpcs_ssl_files.zip.





Exercises

1. (7 points) Add/configure user registration, & authentication/authorization mechanisms. 
We will use the Cork Python module (cork.firelet.net) to implement this functionality. Cork is a very simple, lightweight module that uses a relational database (among other storage types) to store user profile information. The database is accessed via SQLAlchemy (www.sqlalchemy.org/), a widely used Python module for interacting with SQL databases. Cork also makes use of the Beaker Python module (beaker.readthedocs.org)  to store session information (e.g. cookies that contain the username). Cork, SQLAlchemy, and Beaker are already installed on your instance when you use the new AMI in the prep section. Further, Cork requires an SMTP server to send emails confirming a new user’s registration. We will use Amazon Simple Email Service (SES) for this purpose.

I decided to spare you the misery of working with all these modules individually and so I’ve already integrated them into the framework. All you need to do is setup and initialize the database and configure your web app to use them.

(a) (2 points) Set up an RDS MySQL database instance to store user profile information and use for authentication/authorization; you can use the console for this:
Navigate to the RDS console: https://console.aws.amazon.com/rds/home?region=us-east-1
Click “Launch a DB Instance” and select MySQL.
Select the Dev/Test MySQL option.
For the DB Instance Class select db.t2.micro which is sufficient for our testing purposes.
In the interests of saving money, please DO NOT use multi-AZ deployments for RDS.
Leave Allocated Storage as the 5GB default.
Your DB Instance Identifier should include your AWS username, e.g. instructor-auth-db
Provide a master (root) username and password and click “Next Step”
Select “mpcs” as the VPC Security Group(s).
Give your database a name, e.g. gasauth.
Set Backup Retention Period to 0 days (i.e. disable backups). Again, we’re doing this in the interest of saving costs; in practice you would definitely want RDS to make backups for you.
Leave all other values at their defaults and click “Launch DB Instance”.

Give your DB instance about 10-15 minutes to reach the “available” state before continuing.

(b) (2 points) Initialize the user database. Cork uses three tables for authn/authz: users, roles and register. When you execute the following script, it will create these three tables and populate the roles table with some default values. This should be run on your instance (you can run it locally, but you will need the mysql client installed):

mysql --host=<your_rds_endpoint> --port=3306 --user=<your_admin_name> --password --database=gasuath < mpcs_initialize_user_db.sql

The SQL script is at: https://s3.amazonaws.com/mpcs-resources/src/mpcs_initialize_user_db.sql. You only need to run this once; after the database is initialized, all new instances that you launch will simply access it.

(c) (1 point) Edit mpcs.conf and change mpcs.auth.db_url (this is used by Cork to connect to the RDS database you set up in 2(a) above). Cork requires a database URL of the form:

mysql://<rds-admin-name>:<rds-password>@<rds-endpoint:port>/<rds-db-name>

(d) (1 point) Configure your application to use Amazon Simple Email Service (SES) for sending email notifications. Edit mpcs.conf and change mpcs.auth.email_sender to: <your_aws_username>@ucmpcs.org.

(e) (1 point) Modify the views/registration_email.tpl template to include your personal subdomain so that registration confirmations go to the correct URL.

Now that you’ve configured Cork you can use the auth object as follows:

You can access an authenticated user’s attributes as follows (assuming you include:      from mpcs_utils import auth in your source file):
auth.current_user.username - returns the unique username
auth.current_user.role - returns the user’s role
auth.current_user.description - returns the user’s full name
auth.current_user.email_addr - returns the user’s email address

Cork is initialized with the following roles by default:
free_user: has access to pages/functions that require authentication, e.g. the /analysis page.
premium_user: has access to pages only available to Premium users
admin_user: has access to admin pages/functions (not used in this project)
super_user: has access to all system pages/functions (not used in this project)

Make sure you pass auth=auth to all your templates; then you can access user information in the template as: {{auth.current_user.<user-attribute>}}.

If you want to access variables stored in the session, use: auth._beaker_session.get('<session-variable-name>')

You can restrict access to a route/function in the GAS by requiring that a user be authenticated. As the first line in the route add: auth.require(fail_redirect='/login). This redirects unauthenticated users to the “/login” page to authenticate.

You can also specify that a user be redirected back to the requested page after authenticating, by specifying a redirect URL in the authentication check:

auth.require(fail_redirect='/login?redirect_url=' + request.url).

If you want to restrict access to a function/page only to a specific role, use the following:

	auth.require(role=<role-name>, fail_redirect='/login')

	See above for valid role names.

VERY IMPORTANT - Before continuing, start your web server andtest your new setup by going to https://<aws-username>.ucmpcs.org:4433. After consenting to the untrusted certificate security warnings, you should see the GAS home page. Note: You will see security warnings because we’re using a self-signed SSL certificate (not one issued by a trusted certificate authority).



You should be able to:
Register a new user account and receive a confirmation email.
Click on the link in the confirmation email to activate your new account.
Login with your new account and see the options on the header navigation bar change.

If you cannot access the web app and complete these tasks do not continue with other exercises before resolving. Note: you may need to set the file mode on mpcs_run.sh to 755 to make it executable.


2. (12 points) Migrate and update existing code.
(a) (2 points) Move your existing code for uploading an input file to S3 and handling the redirect (saving the job to DynamoDB and publishing a notification for the annotator) into mpcs_app.py.

(b) (4 points) Expose new functionality via the GAS menu system, e.g. add an option to list annotation jobs and to submit a new job (see the base.tpl file on how to do this; basically it’s just entries in a <ul> list).

(c) (6 points) Up until now we’ve hardwired a username for the annotation request. Modify your code so that it uses the auth object to get the authenticated user’s username and include that in the annotation database, as well as anywhere else where the username may be needed. Only authenticated users should be allowed to use the system; the only thing that unauthenticated users can do is view the home page.
3. (16 points) Add mechanisms for handling completion and notification of annotation jobs. We need some way to notify a user when the job is complete (or has encountered an irrecoverable error). We will do this by publishing a notification to an SNS topic (with a subscribed SQS queue, as we have for job requests) and running a separate Python script that sends emails to users.

(a) (1 point) As you did in HW6, create an SNS topic for results notification. The topic name should include your AWS username, e.g. instructor_results_notifications.

(b) (1 point) As you did in HW6, create an SQS queue to store results notification messages.

(c) (1 point) Subscribe your SQS queue to the SNS topic (again, just like you did in HW6).

(d) (3 points) Modify run.py so that it publishes a notification to this topic when the job is complete, i.e. after the database has been updated and the results/log files have been copied to S3. 

(e) (10 points) Write a new Python script called results_notify.py that polls the SQS results queue, and sends an email to the user when their job is complete. This script will be similar to the job_runner.py in HW6, except that it sends an email message instead of launching an annotation job.. In order to send email, use boto to access the Amazon SES service; docs are here: http://boto3.readthedocs.io/en/latest/reference/services/ses.html#ses.

The notification email must include the job ID, linked to the page that displays the job details (see Exercise 5 below). Run the results_notify.py script on a separate instance and tag this instance as <username>-Utilities since we will run other scripts here also.


4. (6 points) Display a list of all jobs for a given user. Create a page that displays a list of all the jobs submitted by a given user; example below:



The page should display a table that contains four columns: the job ID, the date/time submitted, the input file name and the job status. The job ID should be hyperlinked to a page that shows all the details of the job (see next exercise). Make sure you check that the user requesting the jobs list is a valid user and is the user currently authenticated.


5. (6 points) Display job details. Create a page that displays the details of a given job. The page should be rendered in response to a request like: https://<my_gas_url>:4433/analysis/<job-uuid>. It should list these attributes (as retrieved from the annotations database):
Job ID: <UUID of job>
Status: <job status> (i.e. “running”, “completed”, etc.)
Request Time: <date/time when job submitted>
Input filename: <input file name> (this must be a hyperlink that retrieves the input file)
Results filename: <results file name> (this must be a hyperlink that retrieves the results file)
Log filename:<log file name> (this must be a hyperlink that retrieves the log file)
Complete Time: <date/time when job completed>

The page should look something like this:




6. (6 points) Provide a means for users to download results and view log files. When the user clicks one of the filename hyperlinks in the job detail listing (see 5. above) do the following:
If the user clicks on the results file name link, download the file from S3 to the user’s laptop.
If the user clicks on the log file link, display the log file in the browser (hint: you can read the S3 object into a string and just return that in a simple template).

7. (35 points) Integrate Stripe for credit card payment functionality for premium users, and enable Free users to upgrade to Premium.

This exercise is described in a separate doc: https://docs.google.com/document/d/1apVK5gfLzXPUc8N1O9SKQ8hWPtXGh6p_sjs_5Z0mHJU/edit?usp=sharing


8. (15 points) Archive Free user data to Glacier. Our policy for the GAS is that Free users can only download their data for up to 2 hours after completion of their annotation job. After that, their data will be archived to Glacier. This allows us to retain their data at relatively low cost, and to restore it in the event that the user decides to upgrade to a Premium user.

Write a Python script that periodically checks the completion times of jobs belonging to Free users and moves their results files from the gas-results bucket to the gas-archive bucket. I have set a lifecycle policy on the gas-archive bucket that will archive any objects in the bucket immediately**.

There are many ways to accomplish this type of periodic background task, including polling the database, using message queues, using a workflow service like Amazon SWF, etc. In general, polling approaches are simpler but less scalable, while other approaches introduce complexity into the system. For the purposes of this assignment, you can use any approach you like, but you must describe the approach used and your reasons(s) for choosing this approach in the notes you submit with your final project.

Hints: Since you may need to restore archived objects (see (b) below), consider prepending the username (with a delimiter) to the object name so that it’s easy to identify which objects need to be restored. For example, if the log file belonging to user “instructor” has the S3 key name  6b35b815-abaa-4563-a15e-59baf5d6448c.count.log, the archived key name would be something like: sexton#6b35b815-abaa-4563-a15e-59baf5d6448c.count.log.

** Note: “immediately” in this context means that S3 will transition the object to Glacier at some point in the very near future -- no guarantees on an exact time when this will happen, but it’s good enough for our purposes. If we wanted an object moved to Glacier at a specific time, we would need to download the object from S3 and upload it again, directly to Glacier. This introduces other complexities that are not relevant to the project.


9. (10 points) Restore data for users that upgrade to Premium. When a Free user converts to a Premium user, move all of that user’s data from the gas-archives bucket back to the gas-inputs and gas-results buckets. Note that this is a two-step operation:

Restore the object from Glacier to S3 (see this page for information about restoring objects: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOSTrestore.html)
Copy the restored object to the user’s directory in the gas-results bucket, then delete it from the gas-archive bucket. Note that when restoring an archived (Glacier) object, a copy of the object is created in S3 and this copy exists only for a limited period, after which it is deleted. The archived copy persists until explicitly deleted. So, you will need to copy the restored object and then delete it.

Since there is an indeterminate delay between your restoration request and the time that the object is available in S3, you will need to run code in two places:

After the user has upgraded their membership, you must request the object restoration for any results files that were generated while the user was Free user. This will require modifications to your code that processes a subscription request, after the user is created in Stripe.
A separate script must periodically check on the status of restored(ing) objects and move those that are completely restored from the archive bucket to the user’s results bucket.

Once again, there are many ways to accomplish these tasks, each with their own tradeoffs -- you can use any approach you like, but you must describe the approach used and your reasons(s) for choosing this approach in the notes you submit with your final project.


10. (12 points) Add a load balancer for the GAS web servers.

This exercise is described in a separate doc:
https://docs.google.com/document/d/1xj_WxiSpWptswNuOvZPy4zzeD4xYAA76WyvtRQUYzhc/edit?usp=sharing


11. (8 points) Add auto scaling rules to the web server farm. Our Auto Scaling group is currently fixed in size at 2 instances. After extensive profiling and experimentation you’ve discovered that web app response times rise sharply when we’re processing more than 1,000 requests per minute (yes, this is a very unrealistic metric). Your profiling exercise also showed that loads of 100 requests per minute or lower can easily be handled without additional instances. We will now create scaling policies that add and remove instances as load changes.

(a) (2 points) Create a CloudWatch alarm on your load balancer that is triggered when the sum of successful responses exceeds 1,000 for at least one minute. Successful responses are those that return any of the HTTP 2XX status codes. (We’re ignoring HTTP error codes (4XX and 5XX) just for simplicity). CloudWatch alarms are managed on the “Monitoring” tab in the load balancer console. Note: You do not need to send a notification when the alarm is triggered, although in a live application you would definitely want to notify someone and/or log all alarms.

(b) (2 points) Create another CloudWatch alarm on your load balancer that is triggered when the sum of successful responses is below 200 for at least five minutes.

(c) (2 points) Create a policy that scales out your web server farm. Your policy must add one instance when the above alarm is triggered (i.e. when the ELB receives more than 1,000 HTTP 2XX responses in a minute). Policies are managed on the “Scaling Policies” tab of the Auto Scaling Groups console.

(d) (2 points) Create a policy that scales in your web server farm. Your policy must remove one instance when the above alarm is triggered (i.e. when the ELB receives less than 200 HTTP 2XX responses in 5 minutes). Policies are managed on the “Scaling Policies” tab of the Auto Scaling Groups console.

We will put these policies to the test when we turn on the bees with machine guns!


12. (18 points) Add scaling rules to the annotator. So far we’ve focused on the front end of the GAS service. Now we turn our attention to automating and scaling the back end, i.e. the annotator. Scaling the annotator server farm configuration is somewhat simpler than that of the web server farm, since there is no load balancer involved; the auto scaler will simply add and remove instances based on the number of messages arriving in the job requests queue.

(a) (6 points) Preparing the annotator instances for automated launch: This is similar to the automated deployment of the web server instances, but you only need to deploy the AnnTools files, job_runner.py, run.py, and your AWS credentials file -- the SSL certificate files are not required since the annotator instances do not serve HTTPS requests. As before:

Create a ZIP archive containing the annotator files and your AWS credentials (call it gas_annotator.zip) and put it in your home folder on S3; don’t forget to make it public.

Copy auto_scaling_user_data_web_server.txt to a new file, auto_scaling_user_data_annotator.txt, and modify it so that it downloads and unzips the annotator ZIP file. Make sure you change file ownership after copying since the user data commands run as root, e.g. chown -R ubuntu:ubuntu /home/ubuntu/mpcs/...

Add a command to your user data that runs the annotator script. This must be the last command in the file and should look something like:
sudo -u ubuntu python /home/ubuntu/anntools/job_runner.py &

(b) (2 points) Create a launch configuration for annotator instances. You can use the same instructions used for the web server launch configuration, except that the annotator does not require any ports open in its security group since it only calls on other AWS services (SNS, DynamoBD, etc.).

(c) (2 points) Create an auto scaling group for the annotator. You can use the same instructions used for the web server in Exercise 10 above, except that you will not associate an Elastic Load Balancer with this auto scaling group. As previously mentioned, you must use a tag for your auto scaled instances called “Name” with the value “<username>-capstone-annotator”, e.g. instructor-capstone-annotator.

(d) (2 points) Create a CloudWatch alarm to scale out the annotator server farm. Since this alarm is not based on EC2 metrics we will need to create it via the CloudWatch console.
On the CloudWatch console, select “Alarms” and click “Create Alarm”.
Click on “SQS Metrics” and locate your job requests queue.
Select the “NumberOfMessagesReceived” metric and click “Next”.
Set the parameters so that the alarm triggers when more than 10 messages are received by the job request queue in one minute.
Under “Actions”, delete the default notification and click “Create Alarm”.

(e) (2 points) Create another CloudWatch alarm to scale in the annotator server farm when the number of messages received is below 5 for 2 minutes.

(f) (2 points) Create an Auto Scaling policy to scale out the annotator server farm. It must add one annotator instance to the farm when the message queue alarm in 11(d) above is triggered.

(g) (2 points) Create an Auto Scaling policy to scale in the annotator server farm when load subsides. It must remove one annotator instance from the farm when the message queue alarm in 11(e) above is triggered.

13. (9 points) Test under load using Bees with Machine Guns. 

NOTE: THIS EXERCISE IS SUBJECT TO CONFIRMATION SINCE IT IS NOT YET VALIDATED TO WORK WITH BOTO3. DO NOT ATTEMPT TO COMPLETE THIS EXERCISE BEFORE CHECKING WITH ME.

Now that we have our scaling policies set up let’s see how them in action. We will use a simple but effective load testing tool called Bees with Machine Guns, developed by developers at the Chicago Tribune. The project is hosted at: https://github.com/newsapps/beeswithmachineguns. Note that this tool only tests static pages (i.e. we can’t test something like requesting annotations since that requires form submission), but it’s good enough to give you a taste of automated testing tools. We will use it to run some crude experiments on our load balancer.

(a) Update your web server auto scaling configuration. Recall that we set a fixed limit of 2 instances when we created the auto scaling group; our auto scaler will not respond to alarms unless we change this. Go to the Auto Scaling console, select your group, and under the “Details” tab, edit the “Max” to be 10 instances.

(b) Install the tool and configure the AWS credentials as described on the GitHub site above. Note that the tool does not seem to like our standard configuration which stores credentials in ~/.aws/credentials so you may need to create a ~/.boto file and include them there also.

(c) Create a hive with 10 bees (use the mpcs security group and your own EC2 key).

(d) Launch some attacks on your web farm and validate that the scaling policies are enacted, i.e. it scales out and in as load increases and subsides, respectively. An examples of an attack that I used is bees attack -n 100000 -c 1000 -u https://tester.ucmpcs.org/ (note that the trailing “/” is required for the command to work). This example pushed things close to the limit of 10 instances. Capture the output of your tests and include them in your project submission. Also capture some screenshots that show how the instances in your auto scaling group grew and shrank at various points.

VERY IMPORTANT: Please stand down your bee hive as soon as you’re done with testing. We are running really close to the limit on EC2 credits! You can always recreate the hive and run new tests at a later stage.

14. (10 points) Load test the annotator farm. We can’t use Bees to test the annotator since our instances are not exposed to the Internet (and don’t listen for HTTP traffic) but we set the alarm thresholds purposely low in Ex. 11 so we can use a simple script to simulate load.

Write a Python script that submits an arbitrary number of messages to the request queue (simulating job submissions). You can use hardcoded, repetitive test data, i.e. same input file on S3, fixed username, etc. Run your script and see how the auto scaler behaves.

Final Deliverables
On completion of the project you’re expected to submit the following:

A fully-functional GAS accessible at https://<your-username>.ucmpcs.org. Your environment will  include three types of instances:
Instance(s) running the web application
Instance(s) running the annotator
An instance running the utility scripts (

Full source code in a .zip file. Include:
All Python scripts (including the ones provided with the framework)
All templates (including the ones provided with the framework)
Configuration file(s)
AWS EC2 user data files
AWS policy file(s), and any other files necessary for your GAS to run.

Reminders:
Include basic error trapping and handling code for critical actions.
Do not hardcode anything that doesn’t need to be hardcoded; credential hardcoding will result in -10 points.
Please clean up your code (remove unused functions from old homework, etc.).
Add comments, especially for new code, e.g. job listings, file downloads.
If you must submit code in a PDF or .docx file please use a monospaced font.

List all references, ideally as inline comments. Failure to cite references where we expect you to have consulted outside sources will result in -5 points per occurrence.

Include a short write-up that explains any aspect of your work you wish to elaborate on, e.g. the approach used to handle archiving of results files for Free users. In particular, I encourage you to submit notes if you don’t manage to complete all the exercises and wish to describe what issues you encountered that prevented you from doing so.

This is the flow we will use to test your GAS:

Register a new user.
Get email confirmation notice; click to confirm new user registration.
Login using the new username.
Run one or more annotations.
Receive notification email when annotation(s) is(are) complete.
View a list of annotation jobs.
Click on a job in the list and view the details.
Click on a link to download the input/results files, and view the log file (this should work for all Free users right after results are received).
Revisit the list of annotation jobs, after allowing at least two hours to pass. Click on a link to download the input/results files (this should not work; I should be asked to upgrade maybe just be displaying an upgrade link instead of a file link); I should still be able to view the log file.
View the user’s profile; click the link to upgrade to a Premium user.
Provide the credit card info and upgrade to a Premium user.
View the jobs list again.
Click on a link to download the input/results files, and view the log file (this should all work now since it’s a Premium user).

We will check the following in the AWS environment:

Uniquely identified Input, output, log files are stored in the correct S3 buckets.
Correctly updated items are stored in the database for each job.
Archived Free user files are stored in the gas-archive bucket (storage class set to Glacier).
You have correctly configured two notification topics (one for job requests, one for job completions) and subscribed the corresponding message queues to their topic.
You have a running load balancer with two instances in service.
You have two Auto Scaling groups, each with 2 instances: one for the web app and another for the annotator.
All your instances are properly tagged by your auto scalers.
You’re using the mpcsStudents key for all instances.
You have one or two instances running the results notifier and the file archiver scripts.

We will use Da Bees to test the scaling policies of your web auto scaling group. We expect that it will scale up and down under load without exceeding the maximum of 10 instances or going below the minimum of 2 instances. We will use manual loading to test the annotator auto scaling group.
